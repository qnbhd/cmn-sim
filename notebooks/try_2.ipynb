{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef46b2f0-fccf-4d74-b36f-b9ef0dda56ad",
   "metadata": {},
   "source": [
    "Список задач, утверждённых на созвоне:\n",
    "1. Разбиение по группам одинаковых названий компаний\n",
    "2. Разбиение датасета на `train` и `test` части. Часть test  будет балансированной, часть train будет как полной так и сбалансированной.\n",
    "3. Введение мертики map@n\n",
    "4. Выделение фичей в парах слов и обучение на этих фичах классификатора\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bd027a4-fe2d-4381-81d3-7b23e81fd21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3229f8b-3973-4543-bb02-fa1ba1eb956e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"train.csv\", index_col=0)\n",
    "df_test = pd.read_csv(\"test.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01050750-6170-447c-bcc0-826032de4d70",
   "metadata": {},
   "source": [
    "Поскольку мы будем заниматься \"кластеризацией\" названий, я не вижу смысла брать сбалансированный датасет. При желании, вы сами можете взять сбалансированный датасет и загрузить его в переменную `df` внутри следующей ячейки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf26613a-ec78-4f0c-bb9d-8bcc6c893827",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_train, df_test]).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d2fac3-38d0-4acf-9daf-a9b6a360624c",
   "metadata": {},
   "source": [
    "# 1. Разбиение на группы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93f5a4a8-6ab7-4606-9ede-416d961e2b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17814/17814 [04:45<00:00, 62.32it/s]\n"
     ]
    }
   ],
   "source": [
    "all_company_names = list(set(list(df.name_1.values) + list(df.name_2.values)))\n",
    "df_all_pairs_for_name_1 = pd.concat(\n",
    "    [df, df.rename(columns={\"name_1\": \"name_2\", \"name_2\": \"name_1\"})]\n",
    ").drop_duplicates()\n",
    "all_clusters = []\n",
    "\n",
    "for company_name in tqdm(all_company_names):\n",
    "\n",
    "    # Check for company name in clusters\n",
    "    company_in_clusters = company_name in [x for y in all_clusters for x in y]\n",
    "\n",
    "    # Create new cluster:\n",
    "    if not company_in_clusters:\n",
    "        check_list = [company_name]\n",
    "        last_df_len = 0\n",
    "        while True:\n",
    "            df_filter = df_all_pairs_for_name_1\n",
    "            df_filter = df_filter[df_filter.name_1.isin(check_list)]\n",
    "            df_filter = df_filter[df_filter.is_duplicate == 1]\n",
    "            check_list = list(set(list(df_filter.name_2.values) + check_list))\n",
    "            if df_filter.shape[0] > last_df_len:\n",
    "                last_df_len = df_filter.shape[0]\n",
    "            else:\n",
    "                all_clusters.append(check_list)\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6291ef38-e57d-4d9f-8d7e-d0c34dd9704f",
   "metadata": {},
   "source": [
    "Видно, что есть названия которые повторяются в кластерах, но их 300 шт, к сожалению учитывая недостаток времени, придётся доаустить такою погрешность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bd19475-9654-41d8-8704-496203389857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Все названия в кластерах 17814\n",
      "уникальные названия в кластерах 17814\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flaten_clusters = [x for y in all_clusters for x in y]\n",
    "print(\"Все названия в кластерах\", len(flaten_clusters))\n",
    "print(\"уникальные названия в кластерах\", len(set(flaten_clusters)))\n",
    "len(flaten_clusters) == len(set(flaten_clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23dee4e-d83f-4a5c-b1c6-1d1b6d78ae33",
   "metadata": {},
   "source": [
    "Вот пример парочки больших кластеров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ae38677-97fe-459b-a47a-697ffb420e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Basf India Ltd.', 'Basf Japan Ltd. 6 10 1 Roppongi', 'Basf India', 'Basf Construction Chemicals Ua', 'Basf New Zealand Ltd.', 'Basf Sa', 'Basf Chile S.A.', 'Basf Construction Chemicals', 'Basf Corp.', 'Bdp International Basf Imp.', 'Basf Turk Kimya San Ve Tic.Tld.Sti', 'Basf Auxiliary Chemicals', 'Basf Turk Kimya San.Ve Tic Ltd.Sti', 'Basf Peruana S.A.', \"Basf's Paper Chemicals (Huizhou) Co., Ltd.\", 'Basf De Costa Rica Sociedad Anonima', 'Basf Japan Ltd.', 'Basf Co., Ltd. Yeosu', 'Basf Turk Kimya San. Ve Tic.Sti', 'Basf Quimica Colombiana S.A.', 'Basf Bangladesh Ltd.', 'Basf Pakistan Ltd.', 'Basf Co., Ltd.', 'Basf Colors & Effects Usa Llc', 'Basf Mexicana S.A. De C.V.', 'Basf Corporation', 'Basf Finlay Pvt., Ltd.', 'Basf (China) Co., Ltd. Shanghai', 'Basf Chile S A', 'Basf Pakistan (Private) Ltd.'], ['Pirelli Neumaticos S.A. De Cv', 'Pirelli Neumaticos Sa De Cv.', 'Pirelli De Venezuela C.A.', 'Pirelli Neumaticos Sa', 'Pirelli Neumaticos S.A.I.C.', 'Pirelli Neumaticos Argentina Sa', 'Pirelli Tire Llc', 'Pirelli Neumaticos S.A. De C.V.', 'Pirelli Neumaticos Sa De', 'Pirelli Neumaticos Sa De Cv Boulev', 'Pirelli Pneus Ltda', 'Pirelli Tyre Co., Ltd.'], ['Nippon Oil Exploration Limited', 'JX Nippon Oil & Gas Exploration Technical Services Corporation', 'JX Nippon Oil & Gas Exploration (Brasil) Ltda', 'China Southern Petroleum Exploration and Development Corporation', 'Nippon Oil Exploration U.S.A. Limited', 'JX Nippon Oil & Gas Exploration Australia Pty Ltd.', 'JX Nippon Oil & Gas Exploration (Malaysia) Limited', 'JX Nippon Oil & Gas Exploration Corporation', 'Nippon Oil Exploration (Sarawak) Limited', 'JX Nippon Oil & Gas Exploration (Sarawak), Ltd', 'JX Nippon Oil & Gas Exploration (Qatar) Limited', 'JX Nippon Oil & Gas Exploration (Myanmar) Limited']]\n"
     ]
    }
   ],
   "source": [
    "print([c for c in all_clusters if len(c) > 4][:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3324c9-7209-422b-8fdb-e06637dbb738",
   "metadata": {},
   "source": [
    "# 2. Датасеты"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6026e0-004b-431e-8f5c-232a6441e722",
   "metadata": {},
   "source": [
    "Все данные представлены в датафрейме `df`. Для теста возьмём 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f92b68e1-645b-45b1-8965-5bcafdb921c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test_samples = round(df[df.is_duplicate == 1].shape[0] * 0.2)\n",
    "\n",
    "n_balanced_samples = df[df.is_duplicate == 1].shape[0]\n",
    "\n",
    "df_test = pd.concat(\n",
    "    [\n",
    "        df[df.is_duplicate == 1].iloc[:n_test_samples, :],\n",
    "        df[df.is_duplicate == 0].iloc[:n_test_samples, :],\n",
    "    ]\n",
    ")\n",
    "\n",
    "df_test.to_csv(\"test_balanced_dataset.csv\", index=None)\n",
    "\n",
    "df_train = pd.concat(\n",
    "    [\n",
    "        df[df.is_duplicate == 1].iloc[n_test_samples:, :],\n",
    "        df[df.is_duplicate == 0].iloc[n_test_samples:, :],\n",
    "    ]\n",
    ")\n",
    "df_train.to_csv(\"train_unbalanced_dataset.csv\", index=None)\n",
    "\n",
    "df_train_balanced = pd.concat(\n",
    "    [\n",
    "        df[df.is_duplicate == 1].iloc[n_test_samples:, :],\n",
    "        df[df.is_duplicate == 0].iloc[n_test_samples:n_balanced_samples, :],\n",
    "    ]\n",
    ")\n",
    "df_train_balanced.to_csv(\"train_balanced_dataset.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0dea02-1486-4d8e-a5f9-a168652c5980",
   "metadata": {},
   "source": [
    "По итогу получилось:\n",
    "* Сбалансированный test датасет `test_balanced_dataset.csv`\n",
    "* Несбалансированный train датасет, но зато полный `train_unbalanced_dataset.csv`\n",
    "* Сбалансированный test датасет `train_balanced_dataset.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa462ff9-1195-4c04-aace-bc53f0beda5c",
   "metadata": {},
   "source": [
    "# 3. Введение мертики map@n не представляется возможным и вот посему:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc3ed7f-e4d2-4394-ab92-e7f6aad51f3b",
   "metadata": {},
   "source": [
    "...из-за того, что задача под этой метрикой переходит из задачи классификации, в задачу ранжирования.\n",
    "Для метрики, допустим, map@5 нужно иметь список из 5 наиболее релевантных компаний.\n",
    "Поскольку во многих группах куда меньше экземпляров, я не понимаю как это можно реализовать.\n",
    "\n",
    "map@n может применяться в поисковой выдаче, в рекомендательных системах, но в данном случае вопрос как это реализовать остаётся открытым\n",
    "\n",
    "Предлагаю до тех пор использовать f1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54975db-9e19-44df-9690-56e12aafe751",
   "metadata": {},
   "source": [
    "# 4. Обучим классификатор"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd76dc2-61e4-406f-b879-9cb893414899",
   "metadata": {},
   "source": [
    "Для примера генерации фичей возьмём 2 имени из датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2078d022-e7b7-48c0-bfa4-f04ae0721144",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_1, name_2 = df_train.iloc[0, :][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbf83317-036c-420f-aedd-c07a7a641639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Bridgestone Hosepower Llc', 'Bridgestone(Shenyang) Tire Co., Ltd.')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_1, name_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae95f76-a720-4487-b871-00b8142998e2",
   "metadata": {},
   "source": [
    "1. Расстояние леввинштейна"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ff0001b-2913-48ab-a575-a3293e727de4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install python-Levenshtein==0.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6925aa0-07c5-44cd-b413-a57f1dd51823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55a96310-a23b-4265-82cd-23cffbfca7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"levenshtein\"] = [\n",
    "    Levenshtein.distance(n1, n2)\n",
    "    for n1, n2 in zip(df_train.name_1.values, df_train.name_2.values)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b79207f-42c1-47a0-a43a-6f5dfefa1b67",
   "metadata": {},
   "source": [
    "2. Расстояние из библиотеки fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0480e37d-335c-4dbc-ba22-06245ee62c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17f83daa-c9ac-443d-aae8-e7240e7f9a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e3cc8ea-d380-45e6-affe-7e8b9a1a9ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"ratio\"] = [\n",
    "    fuzz.ratio(n1, n2) for n1, n2 in zip(df_train.name_1.values, df_train.name_2.values)\n",
    "]\n",
    "df_train[\"partial_ratio\"] = [\n",
    "    fuzz.partial_ratio(n1, n2)\n",
    "    for n1, n2 in zip(df_train.name_1.values, df_train.name_2.values)\n",
    "]\n",
    "df_train[\"token_sort_ratio\"] = [\n",
    "    fuzz.token_sort_ratio(n1, n2)\n",
    "    for n1, n2 in zip(df_train.name_1.values, df_train.name_2.values)\n",
    "]\n",
    "df_train[\"token_set_ratio\"] = [\n",
    "    fuzz.token_set_ratio(n1, n2)\n",
    "    for n1, n2 in zip(df_train.name_1.values, df_train.name_2.values)\n",
    "]\n",
    "df_train[\"wratio\"] = [\n",
    "    fuzz.WRatio(n1, n2)\n",
    "    for n1, n2 in zip(df_train.name_1.values, df_train.name_2.values)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5989e17-915b-4e50-9788-9bcd91c17b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.iloc[:, 3:].values\n",
    "y_train = df_train.is_duplicate.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd7d91a6-e62f-4cb7-ba30-156e98780100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    df[\"levenshtein\"] = [\n",
    "        Levenshtein.distance(n1, n2)\n",
    "        for n1, n2 in zip(df.name_1.values, df.name_2.values)\n",
    "    ]\n",
    "    df[\"ratio\"] = [\n",
    "        fuzz.ratio(n1, n2) for n1, n2 in zip(df.name_1.values, df.name_2.values)\n",
    "    ]\n",
    "    df[\"partial_ratio\"] = [\n",
    "        fuzz.partial_ratio(n1, n2) for n1, n2 in zip(df.name_1.values, df.name_2.values)\n",
    "    ]\n",
    "    df[\"token_sort_ratio\"] = [\n",
    "        fuzz.token_sort_ratio(n1, n2)\n",
    "        for n1, n2 in zip(df.name_1.values, df.name_2.values)\n",
    "    ]\n",
    "    df[\"token_set_ratio\"] = [\n",
    "        fuzz.token_set_ratio(n1, n2)\n",
    "        for n1, n2 in zip(df.name_1.values, df.name_2.values)\n",
    "    ]\n",
    "    df[\"wratio\"] = [\n",
    "        fuzz.WRatio(n1, n2) for n1, n2 in zip(df.name_1.values, df.name_2.values)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92f2f641-ac7f-45d4-aedc-aa4c91f8e6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_features(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5848a190-128d-43d9-b480-25f321f2a603",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test.iloc[:, 3:].values\n",
    "y_test = df_test.is_duplicate.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312c616a-3b72-4a73-803f-856e3c1bed5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d3f9a31-bbc1-47ec-aea1-86c1700153ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/matching/lib/python3.8/site-packages/xgboost/sklearn.py:793: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "reg = xgb.XGBRegressor(n_estimators=1000)\n",
    "_ = reg.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d10412b-2eec-4521-9363-4183485388eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d71e474-73f2-4154-9d82-efac0879e8a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9bdbe8ff-97b7-42bf-969a-7912ab3cccb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 507.08it/s]\n"
     ]
    }
   ],
   "source": [
    "best_tresh, best_f1 = 0, 0\n",
    "for thresh in tqdm(range(0, 100)):\n",
    "    thresh /= 100\n",
    "    pred = [1 if x > thresh else 0 for x in prediction]\n",
    "    f1 = f1_score(y_test, pred)\n",
    "    if f1 > best_f1:\n",
    "        best_tresh = thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1617157e-a56f-4587-97d0-1fb584335e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.64\n"
     ]
    }
   ],
   "source": [
    "print(best_tresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a65f15e-7bee-4ece-9cb4-ad697c0f9872",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = [1 if x > best_tresh else 0 for x in prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ce52cab-d1cb-4612-98e9-5fa14a47fbd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      1.00      0.68       491\n",
      "           1       1.00      0.06      0.11       491\n",
      "\n",
      "    accuracy                           0.53       982\n",
      "   macro avg       0.76      0.53      0.40       982\n",
      "weighted avg       0.76      0.53      0.40       982\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1db0db4b-3290-4812-a77b-7e5e99236311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11153846153846154"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e91df7f-b17d-4cb8-899b-72a15ad4b3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "714d9325-d342-48a6-b0c2-8aa73e45724e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5295315682281059"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b008be-ed8b-4c58-b461-291c56d36b31",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### F1-score по этому способу: **0.11**\n",
    "### ROC-AUC: **0.53**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c38e99-b945-4451-84eb-3aff7d01952d",
   "metadata": {},
   "source": [
    "# Сравнение со старым решением"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "55d3e5bb-aa42-40aa-b03f-3d2a16411f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "88e6bec9-d83f-4a23-9584-8d66cdb25714",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"char_mapper.pkl\", \"rb\") as f:\n",
    "    char_mapper = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "71053c31-7fe8-4d62-8d61-43b8a65bfc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"test_balanced_dataset.csv\")\n",
    "df_train = pd.read_csv(\"train_unbalanced_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ef1473ec-fd16-46da-977d-9cf97c0a16b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_targets(df):\n",
    "    lowercase = lambda string: string.lower()\n",
    "\n",
    "    name_1 = [i.lower() for i in df.name_1.values]\n",
    "    name_2 = [i.lower() for i in df.name_2.values]\n",
    "\n",
    "    max_len = max(list(map(len, name_1 + name_2)))\n",
    "\n",
    "    # let the blank be 255 chars len\n",
    "    # 164 to map empty cells\n",
    "    name_1_blank = [max(char_mapper.values()) + 1] * 255\n",
    "    name_2_blank = [max(char_mapper.values()) + 1] * 255\n",
    "\n",
    "    def name_to_embedding(name: str) -> list:\n",
    "        embedding = [max(char_mapper.values()) + 1] * 255\n",
    "        for i, char in enumerate(name):\n",
    "            embedding[i] = char_mapper[char]\n",
    "        return embedding\n",
    "\n",
    "    embeddings_list_1 = [name_to_embedding(name) for name in name_1]\n",
    "    embeddings_list_2 = [name_to_embedding(name) for name in name_2]\n",
    "\n",
    "    features = [e1 + e2 for e1, e2 in zip(embeddings_list_1, embeddings_list_2)]\n",
    "    features = np.array(features)\n",
    "\n",
    "    targets = df.is_duplicate.values\n",
    "    return features, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e90ca3e6-f599-4ff4-a48d-513733586fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7a794851-5768-4169-8335-cac46ef54b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = get_features_targets(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "39962d49-024b-4926-baaf-a404a36a2561",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = get_features_targets(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "35ffc917-d285-4fad-98e0-1ea40af25ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = xgb.XGBRegressor(n_estimators=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "64b9b8fc-5b8f-4b20-95d2-5859caaa92f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/matching/lib/python3.8/site-packages/xgboost/sklearn.py:793: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "_ = reg.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "55624c7c-d8c3-4a7b-8567-5ac9d9399b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e81cde5a-af0e-4497-ba74-fb98b53e891b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 513.12it/s]\n"
     ]
    }
   ],
   "source": [
    "best_tresh, best_f1 = 0, 0\n",
    "for thresh in tqdm(range(0, 100)):\n",
    "    thresh /= 100\n",
    "    pred = [1 if x > thresh else 0 for x in prediction]\n",
    "    f1 = f1_score(y_test, pred)\n",
    "    if f1 > best_f1:\n",
    "        best_tresh = thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b0fb700b-4218-4812-a4ef-893d11f128a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = [1 if x > best_tresh else 0 for x in prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8e9680f3-d13e-490a-8bd3-7c8889f1d02e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4313099041533546"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4ce80d01-97c8-4e27-a809-b1a087b82514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6374745417515275"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9906e7-51fa-421c-a61b-575953ff3110",
   "metadata": {},
   "source": [
    "### F1-score по этому способу: **0.43**\n",
    "### ROC-AUC: **0.64**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
